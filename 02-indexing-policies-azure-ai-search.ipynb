{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Prerequisites\n",
    "\n",
    "Ensure that your Azure Services are properly set up, your Conda environment is created, and your environment variables are configured as per the instructions in the [README.md](README.md) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-search-documents==11.6.0b5\n",
    "!pip install python-dotenv\n",
    "!pip install azure-storage-blob\n",
    "!pip install azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "load_dotenv(override=True) # take environment variables from .env."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Env Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Azure Search\n",
    "endpoint = os.environ[\"AZURE_AI_SEARCH_SERVICE_ENDPOINT\"]\n",
    "credential = AzureKeyCredential(os.getenv(\"AZURE_AI_SEARCH_ADMIN_KEY\")) if os.getenv(\"AZURE_AI_SEARCH_ADMIN_KEY\") else DefaultAzureCredential()\n",
    "index_name = os.getenv(\"AZURE_AI_SEARCH_INDEX_NAME\", \"ai-policies-index\")\n",
    "\n",
    "#blob storage\n",
    "blob_connection_string = os.environ[\"BLOB_CONNECTION_STRING\"]\n",
    "search_blob_connection_string = os.getenv(\"SEARCH_BLOB_DATASOURCE_CONNECTION_STRING\", blob_connection_string)\n",
    "blob_container_name = os.getenv(\"BLOB_CONTAINER_NAME\", \"pre-auth-policies\")\n",
    "\n",
    "#Azure OpenAI\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "azure_openai_embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-3-large\")\n",
    "azure_openai_model_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL_NAME\", \"text-embedding-3-large\")\n",
    "azure_openai_model_dimensions = int(os.getenv(\"AZURE_OPENAI_EMBEDDING_DIMENSIONS\", 3072))\n",
    "\n",
    "# This field is only necessary if you want to use OCR to scan PDFs in the data source\n",
    "azure_ai_services_key = os.getenv(\"AZURE_AI_SERVICES_KEY\", \"\")\n",
    "\n",
    "use_ocr = len(azure_ai_services_key) > 0\n",
    "# OCR must be used to add page numbers\n",
    "add_page_numbers = use_ocr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Policies to Blob Storage\n",
    "\n",
    "In this section, we will upload policy documents to Azure Blob Storage. This process involves connecting to the Azure Blob Storage account, creating a container if it doesn't already exist, and uploading the policy documents from a specified local directory to the blob container. \n",
    "\n",
    "### Steps:\n",
    "1. **Initialize Azure Blob Storage Client**: Connect to the Azure Blob Storage account using the connection string and set up the container client.\n",
    "2. **Create Container (if not exists)**: Ensure the specified container exists in the Blob Storage. If it doesn't, create it.\n",
    "3. **Upload Policy Documents**: Iterate through the local directory containing the policy documents and upload each document to the Blob Storage container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:41:06,966 - micro - MainProcess - INFO     Container 'pre-auth-policies' already exists. (blob_helper.py:_create_container_if_not_exists:52)\n"
     ]
    }
   ],
   "source": [
    "from src.storage.blob_helper import AzureBlobUploader\n",
    "\n",
    "uploader = AzureBlobUploader(\n",
    "    connection_string=search_blob_connection_string,\n",
    "    container_name=blob_container_name,\n",
    "    use_user_identity=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:41:07,207 - micro - MainProcess - INFO     Uploaded 'policies_ocr/001_inflammatory_Conditions.pdf' to blob storage. (blob_helper.py:upload_files:90)\n"
     ]
    }
   ],
   "source": [
    "# Local directory to upload files from\n",
    "LOCAL_PATH = r\"C:\\Users\\pablosal\\Desktop\\gbb-ai-hls-factory-prior-auth\\utils\\data\\cases\\policies\"\n",
    "\n",
    "# Remote directory in blob storage\n",
    "REMOTE_PATH = \"policies_ocr\"\n",
    "\n",
    "uploader.upload_files(\n",
    "    local_path=LOCAL_PATH,\n",
    "    remote_path=REMOTE_PATH,\n",
    "    file_filter=AzureBlobUploader.filter_by_extension('.pdf'),\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up a Blob Data Source Connector in Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source 'ai-policies-index-blob' created or updated\n"
     ]
    }
   ],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SearchIndexerDataContainer,\n",
    "    NativeBlobSoftDeleteDeletionDetectionPolicy\n",
    ")\n",
    "\n",
    "# Initialize the SearchIndexerClient\n",
    "indexer_client = SearchIndexerClient(endpoint, credential)\n",
    "\n",
    "# Create a data container for your blob storage\n",
    "container = SearchIndexerDataContainer(name=blob_container_name)\n",
    "\n",
    "# Notes:\n",
    "# - data_change_detection_policy is not applicable for Blob Storage. The indexer automatically detects changes in blobs based on their LastModified timestamps.\n",
    "# - Include data_deletion_detection_policy if needed. Use it to detect deletions, but remember to enable soft delete on your storage account.\n",
    "\n",
    "# Create a data source connection without data_change_detection_policy\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=f\"{index_name}-blob\",\n",
    "    type=\"azureblob\",\n",
    "    connection_string=search_blob_connection_string,\n",
    "    container=container,\n",
    "    data_deletion_detection_policy=NativeBlobSoftDeleteDeletionDetectionPolicy()\n",
    ")\n",
    "\n",
    "# Create or update the data source connection\n",
    "data_source = indexer_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "print(f\"Data source '{data_source.name}' created or updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Search Index\n",
    "\n",
    "A search index is where both vector and non-vector content is stored. This index enables efficient searching and retrieval of documents, allowing for advanced search capabilities and quick access to relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    AzureOpenAIVectorizer,\n",
    "    SemanticConfiguration,\n",
    "    SemanticSearch,\n",
    "    SemanticSearch,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SearchIndex,\n",
    "    HnswParameters,\n",
    "    SimpleField,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexer,\n",
    "    SearchIndexerDataSourceType,\n",
    "    NativeBlobSoftDeleteDeletionDetectionPolicy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a search index  \n",
    "index_client = SearchIndexClient(endpoint=endpoint, credential=credential)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fields\n",
    "fields = [\n",
    "    SearchField(\n",
    "        name=\"parent_id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        sortable=True,\n",
    "        filterable=True,\n",
    "        facetable=True\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"title\",\n",
    "        type=SearchFieldDataType.String,\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"parent_path\",\n",
    "        type=SearchFieldDataType.String,\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"chunk_id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        key=True,\n",
    "        sortable=True,\n",
    "        filterable=True,\n",
    "        facetable=True,\n",
    "        analyzer_name=\"keyword\"\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"chunk\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "        sortable=False,\n",
    "        filterable=False,\n",
    "        facetable=False,\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        vector_search_dimensions=azure_openai_model_dimensions,\n",
    "        vector_search_profile_name=\"myHnswProfile\"\n",
    "    )\n",
    "]\n",
    "\n",
    "if add_page_numbers:\n",
    "    fields.append(\n",
    "        SearchField(name=\"page_number\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=False)\n",
    "    )\n",
    "\n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(  \n",
    "    algorithms=[  \n",
    "        HnswAlgorithmConfiguration(name=\"myHnsw\",\n",
    "                                   parameters=HnswParameters(\n",
    "                                        m=4,\n",
    "                                        ef_construction=400,\n",
    "                                        ef_search=500,\n",
    "                                   )),\n",
    "    ],  \n",
    "    profiles=[  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnsw\",  \n",
    "            vectorizer_name=\"myOpenAI\",  \n",
    "        )\n",
    "    ],  \n",
    "    vectorizers=[  \n",
    "        AzureOpenAIVectorizer(  \n",
    "            vectorizer_name=\"myOpenAI\",  \n",
    "            parameters=AzureOpenAIVectorizerParameters(  \n",
    "                resource_url=azure_openai_endpoint,  \n",
    "                deployment_name=azure_openai_embedding_deployment,\n",
    "                model_name=azure_openai_model_name,\n",
    "                api_key=azure_openai_key,\n",
    "            ),\n",
    "        ),  \n",
    "    ],  \n",
    ")  \n",
    "  \n",
    "semantic_config = SemanticConfiguration(  \n",
    "    name=\"my-semantic-config\",  \n",
    "    prioritized_fields=SemanticPrioritizedFields(  \n",
    "        content_fields=[SemanticField(field_name=\"chunk\")]  \n",
    "    ),  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'ai-policies-index' created or updated successfully.\n"
     ]
    }
   ],
   "source": [
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index\n",
    "index = SearchIndex(\n",
    "    name=index_name,\n",
    "    fields=fields,\n",
    "    vector_search=vector_search,\n",
    "    semantic_search=semantic_search\n",
    ")\n",
    "\n",
    "# Create or update the index\n",
    "index_result = index_client.create_or_update_index(index)\n",
    "print(f\"Index '{index_result.name}' created or updated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Skillset for Integrated Vectorization\n",
    "\n",
    "A skillset in Azure AI Search defines a collection of skills that are applied to your data during indexing. These skills can include text splitting for data chunking, vectorization using Azure OpenAI embeddings, and more. By configuring a skillset, you can enhance your search index with advanced capabilities, making it more efficient and powerful.\n",
    "\n",
    "### Key Components:\n",
    "1. **Text Split Skill**: This skill chunks your data into manageable pieces, improving the granularity and relevance of search results.\n",
    "2. **Azure OpenAI Embedding Skill**: This skill integrates with Azure OpenAI to generate embeddings for your data, enhancing vector search capabilities.\n",
    "3. **Indexer Projection**: Specifies secondary indexes used for chunked data, ensuring that the processed data is correctly indexed and searchable.\n",
    "\n",
    "By setting up a skillset, you can leverage these advanced features to create a more robust and efficient search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a skillset  \n",
    "skillset_name = f\"{index_name}-skillset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SplitSkill,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    OcrSkill,\n",
    "    SearchIndexerIndexProjection,\n",
    "    SearchIndexerIndexProjectionSelector,\n",
    "    SearchIndexerIndexProjectionsParameters,\n",
    "    IndexProjectionMode,\n",
    "    SearchIndexerSkillset,\n",
    "    CognitiveServicesAccountKey\n",
    ")\n",
    "\n",
    "# Create a skillset  \n",
    "skillset_name = f\"{index_name}-skillset\"\n",
    "\n",
    "def create_ocr_skillset():\n",
    "    ocr_skill = OcrSkill(\n",
    "        description=\"OCR skill to scan PDFs and other images with text\",\n",
    "        context=\"/document/normalized_images/*\",\n",
    "        line_ending=\"Space\",\n",
    "        default_language_code=\"en\",\n",
    "        should_detect_orientation=True,\n",
    "        inputs=[\n",
    "            InputFieldMappingEntry(name=\"image\", source=\"/document/normalized_images/*\")\n",
    "        ],\n",
    "        outputs=[\n",
    "            OutputFieldMappingEntry(name=\"text\", target_name=\"text\"),\n",
    "            OutputFieldMappingEntry(name=\"layoutText\", target_name=\"layoutText\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    split_skill = SplitSkill(  \n",
    "        description=\"Split skill to chunk documents\",  \n",
    "        text_split_mode=\"pages\",  \n",
    "        context=\"/document/normalized_images/*\",  \n",
    "        maximum_page_length=3000,  \n",
    "        page_overlap_length=500,  \n",
    "        inputs=[  \n",
    "            InputFieldMappingEntry(name=\"text\", source=\"/document/normalized_images/*/text\"),  \n",
    "        ],  \n",
    "        outputs=[  \n",
    "            OutputFieldMappingEntry(name=\"textItems\", target_name=\"pages\")  \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    embedding_skill = AzureOpenAIEmbeddingSkill(  \n",
    "        description=\"Skill to generate embeddings via Azure OpenAI\",  \n",
    "        context=\"/document/normalized_images/*/pages/*\",  \n",
    "        resource_url=azure_openai_endpoint,  \n",
    "        deployment_name=azure_openai_embedding_deployment,  \n",
    "        model_name=azure_openai_model_name,\n",
    "        dimensions=azure_openai_model_dimensions,\n",
    "        api_key=azure_openai_key,  \n",
    "        inputs=[  \n",
    "            InputFieldMappingEntry(name=\"text\", source=\"/document/normalized_images/*/pages/*\"),  \n",
    "        ],  \n",
    "        outputs=[\n",
    "            OutputFieldMappingEntry(name=\"embedding\", target_name=\"vector\")  \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    index_projections = SearchIndexerIndexProjection(  \n",
    "        selectors=[  \n",
    "            SearchIndexerIndexProjectionSelector(  \n",
    "                target_index_name=index_name,  \n",
    "                parent_key_field_name=\"parent_id\",  \n",
    "                source_context=\"/document/normalized_images/*/pages/*\",  \n",
    "                mappings=[\n",
    "                    InputFieldMappingEntry(name=\"chunk\", source=\"/document/normalized_images/*/pages/*\"),  \n",
    "                    InputFieldMappingEntry(name=\"vector\", source=\"/document/normalized_images/*/pages/*/vector\"),\n",
    "                    InputFieldMappingEntry(name=\"parent_path\", source=\"/document/metadata_storage_path\"),\n",
    "                    InputFieldMappingEntry(name=\"title\", source=\"/document/metadata_storage_name\"),\n",
    "                    InputFieldMappingEntry(name=\"page_number\", source=\"/document/normalized_images/*/pageNumber\")\n",
    "                ]\n",
    "            )\n",
    "        ],  \n",
    "        parameters=SearchIndexerIndexProjectionsParameters(  \n",
    "            projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS  \n",
    "        )  \n",
    "    )\n",
    "\n",
    "    cognitive_services_account = CognitiveServicesAccountKey(key=azure_ai_services_key) if use_ocr else None\n",
    "\n",
    "    skills = [ocr_skill, split_skill, embedding_skill]\n",
    "\n",
    "    return SearchIndexerSkillset(  \n",
    "        name=skillset_name,  \n",
    "        description=\"Skillset to chunk documents and generating embeddings\",  \n",
    "        skills=skills,  \n",
    "        index_projection=index_projections,\n",
    "        cognitive_services_account=cognitive_services_account\n",
    "    )\n",
    "\n",
    "def create_skillset():\n",
    "    split_skill = SplitSkill(  \n",
    "        description=\"Split skill to chunk documents\",  \n",
    "        text_split_mode=\"pages\",  \n",
    "        context=\"/document\",  \n",
    "        maximum_page_length=2000,  \n",
    "        page_overlap_length=500,  \n",
    "        inputs=[  \n",
    "            InputFieldMappingEntry(name=\"text\", source=\"/document/content\"),  \n",
    "        ],  \n",
    "        outputs=[  \n",
    "            OutputFieldMappingEntry(name=\"textItems\", target_name=\"pages\")  \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    embedding_skill = AzureOpenAIEmbeddingSkill(  \n",
    "        description=\"Skill to generate embeddings via Azure OpenAI\",  \n",
    "        context=\"/document/pages/*\",  \n",
    "        resource_url=azure_openai_endpoint,  \n",
    "        deployment_name=azure_openai_embedding_deployment,  \n",
    "        model_name=azure_openai_model_name,\n",
    "        dimensions=azure_openai_model_dimensions,\n",
    "        api_key=azure_openai_key,  \n",
    "        inputs=[  \n",
    "            InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\"),  \n",
    "        ],  \n",
    "        outputs=[\n",
    "            OutputFieldMappingEntry(name=\"embedding\", target_name=\"vector\")  \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    index_projections = SearchIndexerIndexProjection(  \n",
    "        selectors=[  \n",
    "            SearchIndexerIndexProjectionSelector(  \n",
    "                target_index_name=index_name,  \n",
    "                parent_key_field_name=\"parent_id\",  \n",
    "                source_context=\"/document/pages/*\",  \n",
    "                mappings=[\n",
    "                    InputFieldMappingEntry(name=\"chunk\", source=\"/document/pages/*\"),  \n",
    "                    InputFieldMappingEntry(name=\"vector\", source=\"/document/pages/*/vector\"),\n",
    "                    InputFieldMappingEntry(name=\"parent_path\", source=\"/document/metadata_storage_path\"),\n",
    "                    InputFieldMappingEntry(name=\"title\", source=\"/document/metadata_storage_name\"),\n",
    "                ]\n",
    "            )\n",
    "        ],  \n",
    "        parameters=SearchIndexerIndexProjectionsParameters(  \n",
    "            projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS  \n",
    "        )  \n",
    "    )\n",
    "\n",
    "    cognitive_services_account = CognitiveServicesAccountKey(key=azure_ai_services_key) if use_ocr else None\n",
    "\n",
    "    skills = [split_skill, embedding_skill]\n",
    "\n",
    "    return SearchIndexerSkillset(  \n",
    "        name=skillset_name,  \n",
    "        description=\"Skillset to chunk documents and generating embeddings\",  \n",
    "        skills=skills,  \n",
    "        index_projection=index_projections,\n",
    "        cognitive_services_account=cognitive_services_account\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai-policies-index-skillset created\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "try:\n",
    "    skillset = create_ocr_skillset()\n",
    "    client = SearchIndexerClient(endpoint, credential)\n",
    "    client.create_or_update_skillset(skillset)\n",
    "    print(f\"{skillset.name} created\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create skillset: {e.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import IndexingParameters\n",
    "\n",
    "# Configure indexing parameters to include blob metadata\n",
    "indexing_parameters = IndexingParameters(\n",
    "    configuration={\n",
    "        \"parsingMode\": \"default\",\n",
    "        \"indexStorageMetadata\": True\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ai-policies-index-indexer is created and running. If queries return no results, please wait a bit and try again.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexer,\n",
    "    IndexingParameters,\n",
    "    IndexingParametersConfiguration,\n",
    "    BlobIndexerImageAction\n",
    ")\n",
    "\n",
    "# Create an indexer  \n",
    "indexer_name = f\"{index_name}-indexer\"  \n",
    "\n",
    "indexer_parameters = None\n",
    "if use_ocr:\n",
    "    indexer_parameters = IndexingParameters(\n",
    "        configuration=IndexingParametersConfiguration(\n",
    "            image_action=BlobIndexerImageAction.GENERATE_NORMALIZED_IMAGE_PER_PAGE,\n",
    "            query_timeout=None))\n",
    "\n",
    "indexer = SearchIndexer(  \n",
    "    name=indexer_name,  \n",
    "    description=\"Indexer to index documents and generate embeddings\",  \n",
    "    skillset_name=skillset_name,  \n",
    "    target_index_name=index_name,  \n",
    "    data_source_name=data_source.name,\n",
    "    parameters=indexer_parameters\n",
    ")  \n",
    "\n",
    "indexer_client = SearchIndexerClient(endpoint, credential)  \n",
    "indexer_result = indexer_client.create_or_update_indexer(indexer)  \n",
    "  \n",
    "# Run the indexer  \n",
    "indexer_client.run_indexer(indexer_name)  \n",
    "print(f' {indexer_name} is created and running. If queries return no results, please wait a bit and try again.')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(\n",
    "    endpoint=os.environ[\"AZURE_AI_SEARCH_SERVICE_ENDPOINT\"],\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(os.environ[\"AZURE_AI_SEARCH_ADMIN_KEY\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_QUERY = \"Prior Authorization is being requested for the following medication: Adalinumab 40mg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query = VectorizableTextQuery(text=SEARCH_QUERY, k_nearest_neighbors=5, fields=\"vector\", exhaustive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "ID: 5a23e2d0e7e0_aHR0cHM6Ly9zdG9yYWdlZmFjdG9yeWVhc3R1cy5ibG9iLmNvcmUud2luZG93cy5uZXQvcHJlLWF1dGgtcG9saWNpZXMvcG9saWNpZXNfb2NyLzAwMV9pbmZsYW1tYXRvcnlfQ29uZGl0aW9ucy5wZGY1_normalized_images_3_pages_0\n",
      "Reranker Score: 2.91237735748291\n",
      "Source_doc_path: https://storagefactoryeastus.blob.core.windows.net/pre-auth-policies/policies_ocr/001_inflammatory_Conditions.pdf\n",
      "Content: Other Uses with Supportive Evidence There are guidelines and/or published data supporting the use of...\n",
      "Caption: pulmonary and neurosarcoidosis.15 POLICY STATEMENT<em> Prior Authorization</em> is recommended for prescription benefit coverage of<em> adalimumab</em> products All approvals are provided for the duration noted below In cases where the<em> approval</em> is<em> authorized</em> in months, 1 month is equal to 30 days Because of the specialized skills required for evaluation and diagnosi...\n",
      "========================================\n",
      "========================================\n",
      "ID: 5a23e2d0e7e0_aHR0cHM6Ly9zdG9yYWdlZmFjdG9yeWVhc3R1cy5ibG9iLmNvcmUud2luZG93cy5uZXQvcHJlLWF1dGgtcG9saWNpZXMvcG9saWNpZXNfb2NyLzAwMV9pbmZsYW1tYXRvcnlfQ29uZGl0aW9ucy5wZGY1_normalized_images_10_pages_1\n",
      "Reranker Score: 2.80246639251709\n",
      "Source_doc_path: https://storagefactoryeastus.blob.core.windows.net/pre-auth-policies/policies_ocr/001_inflammatory_Conditions.pdf\n",
      "Content: OR Note: An example is prednisone. b) Patient has tried one other immunosuppressant for at least 2 m...\n",
      "Caption: Formulary Coverage - Policy:Inflammatory Conditions -<em> Adalimumab</em> Products<em> Prior Authorization</em> Policy.\u0000 The<em> medication</em> is prescribed by or in consultation with a dermatologist B) Patient is Currently Receiving an<em> Adalimumab</em> Product Approve for 1 year if the patient meets ALL of the<em> following</em> (i, ii, and iii): i therapy or who is restarting therapy w...\n",
      "========================================\n",
      "========================================\n",
      "ID: 5a23e2d0e7e0_aHR0cHM6Ly9zdG9yYWdlZmFjdG9yeWVhc3R1cy5ibG9iLmNvcmUud2luZG93cy5uZXQvcHJlLWF1dGgtcG9saWNpZXMvcG9saWNpZXNfb2NyLzAwMV9pbmZsYW1tYXRvcnlfQ29uZGl0aW9ucy5wZGY1_normalized_images_11_pages_1\n",
      "Reranker Score: 2.7882964611053467\n",
      "Source_doc_path: https://storagefactoryeastus.blob.core.windows.net/pre-auth-policies/policies_ocr/001_inflammatory_Conditions.pdf\n",
      "Content: ONE of the following (A or B): A) Initial Therapy. Approve for 6 months if the patient meets ALL of ...\n",
      "Caption: - Policy:Inflammatory Conditions -<em> Adalimumab</em> Products<em> Prior Authorization</em> Policy.\u0000 iii The<em> medication</em> is prescribed by or in consultation with an ophthalmologist Approve for 6 months if the patient meets ALL of the<em> following</em> (i, ii, and iii): i B) Patient is Currently Receiving an<em> Adalimumab</em> Product Approve for 1 year if the patient meets BOTH of ...\n",
      "========================================\n",
      "========================================\n",
      "ID: 5a23e2d0e7e0_aHR0cHM6Ly9zdG9yYWdlZmFjdG9yeWVhc3R1cy5ibG9iLmNvcmUud2luZG93cy5uZXQvcHJlLWF1dGgtcG9saWNpZXMvcG9saWNpZXNfb2NyLzAwMV9pbmZsYW1tYXRvcnlfQ29uZGl0aW9ucy5wZGY1_normalized_images_3_pages_1\n",
      "Reranker Score: 2.735830307006836\n",
      "Source_doc_path: https://storagefactoryeastus.blob.core.windows.net/pre-auth-policies/policies_ocr/001_inflammatory_Conditions.pdf\n",
      "Content: noted below. In cases where the approval is authorized in months, 1 month is equal to 30 days. Becau...\n",
      "Caption: Yusimryâ„¢ (adalimuamb-aqvh subcutaneous injection - Coherus) is(are) covered as medically necessary when the<em> following</em> criteria is(are) met for FDA-approved indication(s) or other uses with supportive evidence (if applicable): 3 Pages - Cigna National Formulary Coverage - Policy:Inflammatory Conditions -<em> Adalimumab</em> Products<em> Prior Authorization</em> Polic...\n",
      "========================================\n",
      "========================================\n",
      "ID: 5a23e2d0e7e0_aHR0cHM6Ly9zdG9yYWdlZmFjdG9yeWVhc3R1cy5ibG9iLmNvcmUud2luZG93cy5uZXQvcHJlLWF1dGgtcG9saWNpZXMvcG9saWNpZXNfb2NyLzAwMV9pbmZsYW1tYXRvcnlfQ29uZGl0aW9ucy5wZGY1_normalized_images_5_pages_1\n",
      "Reranker Score: 2.72434139251709\n",
      "Source_doc_path: https://storagefactoryeastus.blob.core.windows.net/pre-auth-policies/policies_ocr/001_inflammatory_Conditions.pdf\n",
      "Content: Examples of contraindications to methotrexate include pregnancy, breast feeding, alcoholic liver dis...\n",
      "Caption: Formulary Coverage - Policy:Inflammatory Conditions -<em> Adalimumab</em> Products<em> Prior Authorization</em> Policy.\u0000 The<em> medication</em> is prescribed by or in consultation with a rheumatologist B) Patient is Currently Receiving an<em> Adalimumab</em> Product Approve for 1 year if the patient meets BOTH of the<em> following</em> (i and ii): i Patient has been established on therapy fo...\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.models import QueryType, QueryCaptionType, QueryAnswerType\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=SEARCH_QUERY,  \n",
    "    vector_queries=[vector_query],\n",
    "    #select=[\"content\", \"id\"],\n",
    "    #filters = \n",
    "    query_type=QueryType.SEMANTIC, semantic_configuration_name='my-semantic-config', query_caption=QueryCaptionType.EXTRACTIVE, query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=5\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"ID: {result['chunk_id']}\")\n",
    "    print(f\"Reranker Score: {result['@search.reranker_score']}\")\n",
    "    print(f\"Source_doc_path: {result['parent_path']}\")\n",
    "    content = result['chunk'][:100] + '...' if len(result['chunk']) > 100 else result['chunk']\n",
    "    print(f\"Content: {content}\")\n",
    "\n",
    "    captions = result.get(\"@search.captions\", [])\n",
    "    if captions:\n",
    "        caption = captions[0]\n",
    "        if caption.highlights:\n",
    "            print(f\"Caption: {caption.highlights}\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption.text}\")\n",
    "    print(\"=\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pa-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
